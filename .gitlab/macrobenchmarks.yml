include:
  - project: 'DataDog/benchmarking-platform-tools'
    file: 'images/templates/gitlab/notify-slo-breaches.template.yml'
  - project: 'DataDog/benchmarking-platform-tools'
    file: 'images/templates/gitlab/check-slo-breaches.template.yml'

.macrobenchmarks:
  stage: macrobenchmarks
  rules:
    - if: ($NIGHTLY_BENCHMARKS || $CI_PIPELINE_SOURCE != "schedule") && $CI_COMMIT_REF_NAME == "master"
      when: always
    - when: manual
  tags: ["runner:apm-k8s-same-cpu"]
  needs: []
  interruptible: true
  timeout: 1h
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/benchmarking-platform:js-hapi
  script:
    - git clone --branch rochdev/parallel-experiments https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform platform && cd platform
    - bp-runner bp-runner.$EXPERIMENT.yml --debug -t
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - platform/artifacts/
    expire_in: 3 months
  variables:
    K6_OPTIONS_WARMUP_RATE: 500
    K6_OPTIONS_WARMUP_DURATION: 1m
    K6_OPTIONS_WARMUP_GRACEFUL_STOP: 10s
    K6_OPTIONS_WARMUP_PRE_ALLOCATED_VUS: 4
    K6_OPTIONS_WARMUP_MAX_VUS: 4

    K6_OPTIONS_NORMAL_OPERATION_RATE: 300
    K6_OPTIONS_NORMAL_OPERATION_DURATION: 10m
    K6_OPTIONS_NORMAL_OPERATION_GRACEFUL_STOP: 10s
    K6_OPTIONS_NORMAL_OPERATION_PRE_ALLOCATED_VUS: 4
    K6_OPTIONS_NORMAL_OPERATION_MAX_VUS: 4

    K6_OPTIONS_HIGH_LOAD_RATE: 700
    K6_OPTIONS_HIGH_LOAD_DURATION: 3m
    K6_OPTIONS_HIGH_LOAD_GRACEFUL_STOP: 10s
    K6_OPTIONS_HIGH_LOAD_PRE_ALLOCATED_VUS: 4
    K6_OPTIONS_HIGH_LOAD_MAX_VUS: 4

    DDTRACE_INSTALL_VERSION: "git://github.com/Datadog/dd-trace-js.git#${CI_COMMIT_SHA}"

    CI_JOB_NAME: $CI_JOB_GROUP_NAME

  # Workaround: Currently we're not running the benchmarks on every PR, but GitHub still shows them as pending.
  # By marking the benchmarks as allow_failure, this should go away. (This workaround should be removed once the
  # benchmarks get changed to run on every PR)
  allow_failure: true

  # Retry on Gitlab internal system failures
  retry:
    max: 2
    when:
      - unknown_failure
      - data_integrity_failure
      - runner_system_failure
      - scheduler_failure
      - api_failure

baseline:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: baseline
  parallel:
    matrix:
      - EXPERIMENT: normal-operation
      - EXPERIMENT: high-load

only-tracing:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-tracing
  parallel:
    matrix:
      - EXPERIMENT: normal-operation
      - EXPERIMENT: high-load

only-tracing-with-runtime-metrics-enabled:
  extends: .macrobenchmarks
  variables:
    DD_BENCHMARKS_CONFIGURATION: only-tracing-with-runtime-metrics-enabled
  parallel:
    matrix:
      - EXPERIMENT: normal-operation
      - EXPERIMENT: high-load

check-slo-breaches:
  extends: .check-slo-breaches
  stage: macrobenchmarks-gate
  needs:
    - baseline
    - only-tracing
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - platform/artifacts/
    expire_in: 3 months

notify-slo-breaches:
  extends: .notify-slo-breaches
  stage: macrobenchmarks-notify
  needs:
    - check-slo-breaches
  variables:
    CHANNEL: "notifications-apm-js"
